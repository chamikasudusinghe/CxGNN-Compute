{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangkz/.venvs/fresh/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuner lazy True\n",
      "Loading cache table\n",
      "dataset prepared torch.Size([157659]) torch.Size([1595972]) torch.Size([1425008, 384]) tensor([   1000,   11755,  157658, 1425008])\n",
      "dataset prepared torch.Size([162129]) torch.Size([1640952]) torch.Size([1461853, 384]) tensor([   1000,   12107,  162128, 1461853])\n",
      "dataset prepared torch.Size([151527]) torch.Size([1531929]) torch.Size([1372459, 384]) tensor([   1000,   11375,  151526, 1372459])\n",
      "dataset prepared torch.Size([146311]) torch.Size([1478264]) torch.Size([1324591, 384]) tensor([   1000,   11028,  146310, 1324591])\n"
     ]
    }
   ],
   "source": [
    "import cxgnncomp as cxgc\n",
    "import numpy as np\n",
    "import torch\n",
    "from os import path\n",
    "\n",
    "dset = \"friendster\"\n",
    "infeat = 384\n",
    "num_device = 4\n",
    "\n",
    "total_num_node = int(\n",
    "    open(path.join(f\"../../../../data/{dset}/processed\", \"num_nodes.txt\")).readline())\n",
    "total_num_node = (total_num_node + num_device - 1) // num_device * num_device\n",
    "assert total_num_node % 4 == 0\n",
    "\n",
    "batches = []\n",
    "\n",
    "for i in range(num_device):\n",
    "    feat, ptr, idx, b = cxgc.prepare_graph(\n",
    "        dset=dset,\n",
    "        feat_len=infeat,\n",
    "        num_head=1,\n",
    "        num_seeds=1000,\n",
    "        is_full_graph=0,\n",
    "        need_edge_index=False,\n",
    "        device=i)\n",
    "    feat = None\n",
    "    batches.append(cxgc.Batch(x=None, ptr=ptr, idx=idx, num_node_in_layer=b[\"num_node_in_layer\"]))\n",
    "    batches[-1].ptrs = [batches[-1].ptr.to(dev) for dev in range(num_device)]\n",
    "    batches[-1].idxs = [batches[-1].idx.to(dev) for dev in range(num_device)]\n",
    "    batches[-1].sub_to_fulls = [b[\"sub_to_full\"].to(dev) for dev in range(num_device)]\n",
    "local_feats = [torch.randn(total_num_node, infeat // num_device, device=i) for i in range(num_device)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cxgnncomp_backend\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, num_device, total_num_node, infeat, local_feats, type=\"ddp\"):\n",
    "        self.num_device = num_device\n",
    "        self.total_num_node = total_num_node\n",
    "        self.infeat = infeat\n",
    "        self.type = type\n",
    "        if type == \"ddp\":\n",
    "            self.local_starts = [i * total_num_node // num_device for i in range(num_device)]\n",
    "            self.local_ends = [(i + 1) * total_num_node // num_device for i in range(num_device)]\n",
    "            self.local_feats = [item.reshape(-1, infeat) for item in local_feats]\n",
    "            # self.local_feats = [torch.randn(self.local_ends[i] - self.local_starts[i], infeat, device=i) for i in range(num_device)]\n",
    "            self.convs = []\n",
    "        elif type in [\"tp\", \"opt\"]:\n",
    "            # self.local_feats = [torch.randn(total_num_node // num_device, infeat, device=i) for i in range(num_device)]\n",
    "            self.local_starts = [i * infeat // num_device for i in range(num_device)]\n",
    "            self.local_ends = [(i + 1) * infeat // num_device for i in range(num_device)]\n",
    "            # self.local_feats = [torch.randn(self.total_num_node, self.local_ends[i] - self.local_starts[i], device=i) for i in range(num_device)]\n",
    "            self.local_feats = [item.reshape(-1, infeat // num_device) for item in local_feats]\n",
    "            # self.weights = [torch.randn(infeat, infeat, device=i) for i in range(num_device)]\n",
    "    \n",
    "    def set_hidden(self, hidden):\n",
    "        self.hidden = hidden\n",
    "        if self.type in [\"ddp\", \"opt\"]:\n",
    "            self.weights = [torch.randn(self.infeat, hidden, device=i) for i in range(self.num_device)]\n",
    "        elif self.type == \"tp\":\n",
    "            self.weights = [torch.randn(self.infeat // self.num_device, hidden, device=i) for i in range(self.num_device)]\n",
    "\n",
    "    def generate_x(self, batches):\n",
    "        if self.type == \"ddp\":\n",
    "            self.generate_x_ddp(batches)\n",
    "        elif self.type == \"tp\":\n",
    "            self.generate_x_tp(batches)\n",
    "        elif self.type == \"opt\":\n",
    "            self.generate_x_opt(batches)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    def generate_x_ddp(self, batches):\n",
    "        for i, batch in enumerate(batches):\n",
    "            feats = []\n",
    "            for j in range(self.num_device):\n",
    "                # j -> i\n",
    "                torch.cuda.set_device(j)\n",
    "                sub_to_full = batch.sub_to_fulls[j]\n",
    "                needed = sub_to_full[torch.logical_and(sub_to_full >= self.local_starts[j], sub_to_full < self.local_ends[j])]\n",
    "                feats.append(self.local_feats[j][needed - self.local_starts[j]].to(i))\n",
    "            torch.cuda.set_device(i)\n",
    "            batch.x = torch.cat(feats, dim=0)\n",
    "\n",
    "        for i, batch in enumerate(batches):\n",
    "            torch.cuda.set_device(i)\n",
    "            if self.infeat > self.hidden:\n",
    "                x = torch.mm(batch.x, self.weights[i])\n",
    "                batch.x = cxgnncomp_backend.sage_sum_forward(\n",
    "                    x,\n",
    "                    batch.ptrs[i],\n",
    "                    batch.idxs[i],\n",
    "                    batch.num_node_in_layer[-2]\n",
    "                )\n",
    "            else:\n",
    "                x = cxgnncomp_backend.sage_sum_forward(\n",
    "                    batch.x,\n",
    "                    batch.ptrs[i],\n",
    "                    batch.idxs[i],\n",
    "                    batch.num_node_in_layer[-2]\n",
    "                )\n",
    "                batch.x = torch.mm(x, self.weights[i])\n",
    "\n",
    "        \n",
    "\n",
    "    def generate_x_tp(self, batches):\n",
    "        outputs = []\n",
    "        for tar_it in range(self.num_device):\n",
    "            arr_node_feat = []\n",
    "            for dev_it in range(self.num_device):\n",
    "                torch.cuda.set_device(dev_it)\n",
    "                out = torch.index_select(\n",
    "                    self.local_feats[dev_it],\n",
    "                    dim=0,\n",
    "                    index=batches[tar_it].sub_to_fulls[dev_it]\n",
    "                )\n",
    "                out = cxgnncomp_backend.sage_sum_forward(\n",
    "                    out,\n",
    "                    batches[tar_it].ptrs[dev_it],\n",
    "                    batches[tar_it].idxs[dev_it],\n",
    "                    batches[tar_it].num_node_in_layer[-2]\n",
    "                )\n",
    "                out = torch.mm(out, self.weights[dev_it])\n",
    "                # arr_node_feat[tar_it][dev_it] = out\n",
    "                arr_node_feat.append(out)\n",
    "            batches[tar_it].x = arr_node_feat[tar_it]\n",
    "            for dev_it in range(self.num_device):\n",
    "                if dev_it == tar_it:\n",
    "                    continue\n",
    "                batches[tar_it].x += arr_node_feat[dev_it].to(tar_it)\n",
    "\n",
    "    def generate_x_opt(self, batches):\n",
    "        for tar_it in range(self.num_device):\n",
    "            arr_node_feat = []\n",
    "            for dev_it in range(self.num_device):\n",
    "                torch.cuda.set_device(dev_it)\n",
    "                out = torch.index_select(\n",
    "                    self.local_feats[dev_it],\n",
    "                    dim=0,\n",
    "                    index=batches[tar_it].sub_to_fulls[dev_it]\n",
    "                )\n",
    "                out = cxgnncomp_backend.sage_sum_forward(\n",
    "                    out,\n",
    "                    batches[tar_it].ptrs[dev_it],\n",
    "                    batches[tar_it].idxs[dev_it],\n",
    "                    batches[tar_it].num_node_in_layer[-2]\n",
    "                )\n",
    "                # out = torch.mm(out, self.weights[dev_it])\n",
    "                arr_node_feat.append(out)\n",
    "            # batches[tar_it].x = arr_node_feat[tar_it]\n",
    "            collect_feat = [] \n",
    "            for dev_it in range(self.num_device):\n",
    "                collect_feat.append(arr_node_feat[dev_it].to(tar_it))\n",
    "                # batches[tar_it].x += arr_node_feat[dev_it].to(tar_it)\n",
    "            batches[tar_it].x = torch.cat(collect_feat, dim=1)\n",
    "            batches[tar_it].x = torch.mm(batches[tar_it].x, self.weights[tar_it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt\n",
      "(37.94892883300781, 37.94432067871094, 37.95353698730469)\n",
      "(38.2740478515625, 38.265445709228516, 38.282649993896484)\n",
      "(39.10041427612305, 39.08628463745117, 39.11454391479492)\n",
      "(40.94464111328125, 40.83773422241211, 41.051544189453125)\n",
      "(45.428733825683594, 45.319374084472656, 45.5380973815918)\n",
      "(58.4007682800293, 58.4007682800293, 58.4007682800293)\n",
      "tensor([[ -36.2889,   61.6108,   32.8701,  ...,   -1.5449,  -41.2632,\n",
      "           -8.6654],\n",
      "        [  18.5453,  -53.3738,   33.6478,  ...,  -70.9697,   46.8874,\n",
      "          -68.3772],\n",
      "        [ -17.5429,  -15.9698,  -30.8200,  ...,  -37.9601,  -12.1896,\n",
      "           -5.4655],\n",
      "        ...,\n",
      "        [ 116.5622,  -53.3625,  -10.4726,  ...,   29.2781,   65.5850,\n",
      "          -49.5062],\n",
      "        [ 122.8694, -110.2708, -115.7789,  ...,   53.3824,   58.5736,\n",
      "           12.2467],\n",
      "        [  33.4942,   16.0401,   14.8758,  ...,   10.3647,    3.9851,\n",
      "          142.4099]], device='cuda:0')\n",
      "ddp\n",
      "(270.7476501464844, 270.7476501464844, 270.7476501464844)\n",
      "(272.6369323730469, 272.6369323730469, 272.6369323730469)\n",
      "(276.6561279296875, 276.6561279296875, 276.6561279296875)\n",
      "(285.5331726074219, 285.5331726074219, 285.5331726074219)\n",
      "(275.6597900390625, 275.6597900390625, 275.6597900390625)\n",
      "(278.5208435058594, 278.5208435058594, 278.5208435058594)\n",
      "tensor([[   3.4655,  -47.2152,   24.7245,  ...,  -15.8816,  -57.5619,\n",
      "           64.2216],\n",
      "        [ -44.7151,  -60.3183,   -4.6928,  ...,  -41.6209,   58.0714,\n",
      "          147.5199],\n",
      "        [  25.8888,  -13.9913,  -58.1246,  ...,   14.4014,   18.0277,\n",
      "           22.0352],\n",
      "        ...,\n",
      "        [ -21.6191,   76.6007,  101.2529,  ...,   50.0683,  -34.7912,\n",
      "            5.8345],\n",
      "        [ -36.5844,    2.7628,   33.6785,  ...,   -5.1358,   41.2391,\n",
      "           51.3756],\n",
      "        [ -21.5501,  -75.9731,  -49.9498,  ...,   48.0314,   -3.5500,\n",
      "         -107.8873]], device='cuda:0')\n",
      "tp\n",
      "(21.40108871459961, 21.393407821655273, 21.4149112701416)\n",
      "(29.948928833007812, 29.92619514465332, 30.17011260986328)\n",
      "(46.79679870605469, 46.793113708496094, 46.80048751831055)\n",
      "(80.5898208618164, 80.5898208618164, 80.5898208618164)\n",
      "(149.77023315429688, 149.77023315429688, 149.77023315429688)\n",
      "(286.6800537109375, 286.6800537109375, 286.6800537109375)\n",
      "tensor([[  23.1149,  -12.1492,  -26.4774,  ...,   20.0621,    0.7195,\n",
      "           25.3112],\n",
      "        [  -6.2969,  -82.1614,  -77.0126,  ...,  -49.9887,   -0.8269,\n",
      "           38.6657],\n",
      "        [ -89.6469,   47.8509,  -69.0811,  ...,  -20.1542,   14.8412,\n",
      "          -24.5503],\n",
      "        ...,\n",
      "        [ -49.2362,  -25.4472,   80.3270,  ...,   75.4929,   31.9483,\n",
      "         -143.5442],\n",
      "        [  27.5680,   26.6802,  -71.1354,  ...,  -64.3541, -120.0446,\n",
      "           39.5041],\n",
      "        [ -37.9748,  -68.5262,  -20.0890,  ...,  140.4242,  -82.5206,\n",
      "           81.2989]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for type in [\"opt\", \"ddp\", \"tp\"]:\n",
    "    hidden = 32\n",
    "    print(type)\n",
    "    trainer = Trainer(num_device, total_num_node, infeat, local_feats, type=type)\n",
    "    # trainer.generate_x(batches)\n",
    "    # print(\"dgl\\tp3\\tNollie\")\n",
    "    while hidden <= 1024:\n",
    "        trainer.set_hidden(hidden)\n",
    "        t = cxgc.prof(\"dgl\", \"ddp\", lambda: trainer.generate_x(batches), display=False)[0]\n",
    "        print(t)\n",
    "        if hidden == 1024:\n",
    "            print(batches[0].x)\n",
    "        # P3:\n",
    "        # dgl = num_src * infeat * (num_device - 1) / num_device\n",
    "        # p3 = num_dst * hidden * (num_device - 1)\n",
    "        # our = num_dst * min(hidden * (num_device - 1), infeat * (num_device - 1) / num_device) \n",
    "        # print(f\"{dgl}\\t{p3}\\t{our}\\t{hidden}\")\n",
    "        hidden *= 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
